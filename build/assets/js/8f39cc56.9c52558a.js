"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[314],{5535:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"modules/m4-vla/m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety.","source":"@site/docs/modules/m4-vla/m4-cognitive-planning.mdx","sourceDirName":"modules/m4-vla","slug":"/modules/m4-vla/m4-cognitive-planning","permalink":"/docs/modules/m4-vla/m4-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/JaveriaNigar/Physical-AI---Humanoid-Robotics/docs/modules/m4-vla/m4-cognitive-planning.mdx","tags":[],"version":"current","frontMatter":{"id":"m4-cognitive-planning","title":"Lesson 2: Cognitive Planning with LLMs","sidebar_label":"L2: Cognitive Planning","description":"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety."},"sidebar":"tutorialSidebar","previous":{"title":"L1: Voice-to-Action","permalink":"/docs/modules/m4-vla/m4-voice-to-action"},"next":{"title":"L3: Capstone Project","permalink":"/docs/modules/m4-vla/m4-capstone-project"}}');var t=r(4848),s=r(8453);const o={id:"m4-cognitive-planning",title:"Lesson 2: Cognitive Planning with LLMs",sidebar_label:"L2: Cognitive Planning",description:"Use large language models to translate natural language into step-by-step robot plans. Handle ambiguity, ask clarifications, ensure safety."},i=void 0,l={},c=[{value:"LLM-Based Task Planning",id:"llm-based-task-planning",level:2},{value:"Using Llama 3 Locally",id:"using-llama-3-locally",level:2},{value:"Task Planning Node",id:"task-planning-node",level:2},{value:"Example Plan Output",id:"example-plan-output",level:2},{value:"Handling Ambiguity",id:"handling-ambiguity",level:2},{value:"Safety-Aware Planning",id:"safety-aware-planning",level:2},{value:"Next Lesson",id:"next-lesson",level:2}];function p(n){const e={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{id:"llm-based-task-planning",children:"LLM-Based Task Planning"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Problem"}),': "Pick up the red cube and place it on the shelf"']}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"What an LLM can do"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Break down into steps (pick \u2192 place)"}),"\n",(0,t.jsx)(e.li,{children:"Reason about spatial relationships (red cube, shelf)"}),"\n",(0,t.jsx)(e.li,{children:"Handle ambiguity (ask clarifying questions)"}),"\n",(0,t.jsx)(e.li,{children:"Plan contingencies (what if object is not found?)"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"using-llama-3-locally",children:"Using Llama 3 Locally"}),"\n",(0,t.jsx)(e.p,{children:"Install Ollama and run Llama 3:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Download Ollama\r\nhttps://ollama.ai\r\n\r\n# Pull Llama 3 model\r\nollama pull llama3\r\n\r\n# Start server (runs on localhost:11434)\r\nollama serve\n"})}),"\n",(0,t.jsx)(e.h2,{id:"task-planning-node",children:"Task Planning Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport requests\r\nimport json\r\n\r\nclass TaskPlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'task_planner\')\r\n\r\n        # Subscriber\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/voice/command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher\r\n        self.plan_pub = self.create_publisher(String, \'/robot/plan\', 10)\r\n\r\n        # Ollama endpoint\r\n        self.ollama_url = "http://localhost:11434/api/generate"\r\n        self.model = "llama3"\r\n\r\n        self.get_logger().info("Task planner initialized")\r\n\r\n    def command_callback(self, msg):\r\n        """Process voice command and create task plan."""\r\n        command = msg.data\r\n        self.get_logger().info(f"Planning task: {command}")\r\n\r\n        # Use LLM to create plan\r\n        plan = self.create_plan_with_llm(command)\r\n\r\n        # Publish plan\r\n        plan_msg = String()\r\n        plan_msg.data = json.dumps(plan)\r\n        self.plan_pub.publish(plan_msg)\r\n\r\n    def create_plan_with_llm(self, command: str) -> dict:\r\n        """Generate step-by-step plan using Llama 3."""\r\n\r\n        prompt = f"""\r\nYou are a robot task planner. A humanoid robot receives this command:\r\n\r\n"{command}"\r\n\r\nThe robot has:\r\n- A 6-DOF arm with gripper\r\n- Cameras for vision\r\n- VSLAM for localization\r\n- Nav2 for navigation\r\n- ROS 2 controllers\r\n\r\nGenerate a JSON plan with the following structure:\r\n{{\r\n    "task": "task name",\r\n    "steps": [\r\n        {{\r\n            "step_number": 1,\r\n            "description": "First step",\r\n            "ros_action": "move_arm" | "navigate" | "grasp" | "release",\r\n            "parameters": {{}},\r\n            "preconditions": "what must be true before this step",\r\n            "postconditions": "what will be true after this step"\r\n        }},\r\n        ...\r\n    ],\r\n    "clarifications_needed": [],\r\n    "safety_constraints": []\r\n}}\r\n\r\nIf the command is ambiguous, add clarification questions.\r\nAlways include safety checks.\r\n"""\r\n\r\n        try:\r\n            response = requests.post(\r\n                self.ollama_url,\r\n                json={\r\n                    "model": self.model,\r\n                    "prompt": prompt,\r\n                    "stream": False,\r\n                    "temperature": 0.3  # Deterministic for planning\r\n                },\r\n                timeout=30\r\n            )\r\n\r\n            output = response.json()["response"]\r\n\r\n            # Extract JSON from response\r\n            import re\r\n            json_match = re.search(r\'\\{.*\\}\', output, re.DOTALL)\r\n            if json_match:\r\n                plan = json.loads(json_match.group())\r\n                return plan\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"LLM error: {e}")\r\n\r\n        return {"error": "Failed to generate plan"}\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    planner = TaskPlannerNode()\r\n    rclpy.spin(planner)\r\n    planner.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"example-plan-output",children:"Example Plan Output"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Command"}),': "Pick up the red cube from the table and place it on the shelf"']}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Generated Plan"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-json",children:'{\r\n    "task": "Place red cube on shelf",\r\n    "steps": [\r\n        {\r\n            "step_number": 1,\r\n            "description": "Navigate to table",\r\n            "ros_action": "navigate",\r\n            "parameters": {\r\n                "target_location": "table_front",\r\n                "max_speed": 0.3\r\n            },\r\n            "preconditions": "Robot localized, navigation available",\r\n            "postconditions": "Robot near table, ready to perceive object"\r\n        },\r\n        {\r\n            "step_number": 2,\r\n            "description": "Look for red cube using camera",\r\n            "ros_action": "perception",\r\n            "parameters": {\r\n                "object_to_find": "red cube",\r\n                "color": "red",\r\n                "method": "yolo_detection"\r\n            },\r\n            "preconditions": "Camera operational, scene illuminated",\r\n            "postconditions": "Cube location known in 3D space"\r\n        },\r\n        {\r\n            "step_number": 3,\r\n            "description": "Approach cube for grasping",\r\n            "ros_action": "move_arm",\r\n            "parameters": {\r\n                "target_pose_type": "approach",\r\n                "distance_from_object": 0.05\r\n            },\r\n            "preconditions": "Cube detected, no obstacles",\r\n            "postconditions": "Gripper 5cm from cube"\r\n        },\r\n        {\r\n            "step_number": 4,\r\n            "description": "Grasp cube with controlled force",\r\n            "ros_action": "grasp",\r\n            "parameters": {\r\n                "gripper_force": 50,\r\n                "force_unit": "newtons"\r\n            },\r\n            "preconditions": "Gripper in grasp position, no obstruction",\r\n            "postconditions": "Cube grasped, force feedback received"\r\n        },\r\n        {\r\n            "step_number": 5,\r\n            "description": "Lift cube to safe height",\r\n            "ros_action": "move_arm",\r\n            "parameters": {\r\n                "direction": "up",\r\n                "distance": 0.2\r\n            },\r\n            "preconditions": "Cube grasped, arm ready",\r\n            "postconditions": "Cube elevated, clear of table"\r\n        },\r\n        {\r\n            "step_number": 6,\r\n            "description": "Navigate to shelf location",\r\n            "ros_action": "navigate",\r\n            "parameters": {\r\n                "target_location": "shelf_base"\r\n            },\r\n            "preconditions": "Cube lifted, balance maintained",\r\n            "postconditions": "Robot positioned at shelf"\r\n        },\r\n        {\r\n            "step_number": 7,\r\n            "description": "Place cube on shelf",\r\n            "ros_action": "release",\r\n            "parameters": {\r\n                "target_height": "shelf_top",\r\n                "release_force": 10\r\n            },\r\n            "preconditions": "Gripper above shelf, no obstacles",\r\n            "postconditions": "Cube on shelf, gripper open"\r\n        }\r\n    ],\r\n    "clarifications_needed": [],\r\n    "safety_constraints": [\r\n        "Maintain balance during cube manipulation",\r\n        "Check for obstacles in path to shelf",\r\n        "Verify gripper force is within limits",\r\n        "Confirm shelf can support cube weight"\r\n    ]\r\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class DisambiguationEngine(Node):\r\n    def ask_clarification(self, clarifications: list) -> dict:\r\n        """Ask user for clarification when command is ambiguous."""\r\n\r\n        if not clarifications:\r\n            return {}\r\n\r\n        responses = {}\r\n\r\n        for clarification in clarifications:\r\n            self.get_logger().info(f"Clarification needed: {clarification}")\r\n\r\n            # In real system, would wait for user input\r\n            # For now, use defaults\r\n            if "location" in clarification.lower():\r\n                responses["location"] = "default_location"\r\n            elif "object" in clarification.lower():\r\n                responses["object"] = "largest_available_object"\r\n\r\n        return responses\n'})}),"\n",(0,t.jsx)(e.h2,{id:"safety-aware-planning",children:"Safety-Aware Planning"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class SafetyAwarePlanner:\r\n    def validate_plan(self, plan: dict) -> bool:\r\n        \"\"\"Validate plan for safety violations.\"\"\"\r\n\r\n        unsafe_keywords = ['destroy', 'break', 'hurt', 'harm', 'dangerous']\r\n\r\n        for step in plan.get('steps', []):\r\n            description = step.get('description', '').lower()\r\n            for keyword in unsafe_keywords:\r\n                if keyword in description:\r\n                    return False\r\n\r\n        # Check preconditions\r\n        for step in plan.get('steps', []):\r\n            preconditions = step.get('preconditions', '')\r\n            if not self.can_satisfy_preconditions(preconditions):\r\n                return False\r\n\r\n        return True\r\n\r\n    def can_satisfy_preconditions(self, preconditions: str) -> bool:\r\n        \"\"\"Check if preconditions can be satisfied.\"\"\"\r\n        # In real system, query robot state\r\n        # For demo, assume yes\r\n        return True\n"})}),"\n",(0,t.jsx)(e.h2,{id:"next-lesson",children:"Next Lesson"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"/docs/modules/m4-vla/m4-capstone-project",children:"Lesson 3: Capstone Project"})})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>i});var a=r(6540);const t={},s=a.createContext(t);function o(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);